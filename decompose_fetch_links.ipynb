{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['claim', 'claim_factcheck_url', 'claim_author', 'claim_source',\n",
       "        'claim_date', 'fact_check_date', 'justification',\n",
       "        'fact_checking_sources', 'issue', 'label'],\n",
       "       dtype='object'),\n",
       " 3377)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Question generation prompt\n",
    "with open(\"prompts/questions_template_v2.txt\", \"r\") as f:\n",
    "    question_prompt_template = f.read()\n",
    "\n",
    "# Save all claims and search results\n",
    "save_folder = \"data\"\n",
    "\n",
    "df = pd.read_csv('data/fnd_politifact_claims.csv')\n",
    "df.columns, len(df)\n",
    "\n",
    "# Sample 5 random claims\n",
    "# df = df.sample(1500).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Input format: Month Day, Year\n",
    "# Output format: YYYY-MM-DD\n",
    "def extract_and_format_date(check_date, default_date=\"January 1, 2024\"):\n",
    "    # If the date is not provided, use the default date\n",
    "    if check_date != \"UNKNOWN\":\n",
    "        date_str = check_date\n",
    "    else:\n",
    "        date_str = default_date\n",
    "\n",
    "    # Convert the date string to a datetime object\n",
    "    date_obj = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "\n",
    "    # Format the date as YYYY-MM-DD\n",
    "    formatted_date = date_obj.strftime(\"%Y-%m-%d\")\n",
    "    return formatted_date\n",
    "\n",
    "def parse_gemini_response(response_text):\n",
    "    json_str = response_text.split('```json\\n')[1].split('\\n```')[0]\n",
    "    info = json.loads(json_str)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2684 claims already processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abz/mambaforge/envs/thesis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.gemini_interface_parallel import batch_process\n",
    "\n",
    "secrets_file = \"secrets/gemini_keys_new.json\"\n",
    "model_name = \"gemini-2.0-flash-exp\"\n",
    "# model_name = \"gemini-1.5-flash-latest\"\n",
    "temperature = 0.75\n",
    "top_p = 0.95\n",
    "\n",
    "# First part: Generate queries for all claims\n",
    "claim_queries_filename = f\"{save_folder}/claim_queries_v2.json\"\n",
    "try:    \n",
    "    with open(claim_queries_filename, \"r\", encoding=\"utf-8\") as fp:\n",
    "        claim_queries = json.load(fp)\n",
    "except FileNotFoundError:\n",
    "    claim_queries = {}\n",
    "\n",
    "print(f\"{len(claim_queries)} claims already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    claim = row[\"claim\"]\n",
    "    if claim in claim_queries:\n",
    "        continue\n",
    "    \n",
    "    author = row[\"claim_author\"].strip() if \"claim_author\" in row and row[\"claim_author\"] else \"UNKNOWN\"\n",
    "    claim_date = row[\"claim_date\"].strip() if \"claim_date\" in row and row[\"claim_date\"] else \"UNKNOWN\"\n",
    "    location_ISO_code = row[\"location_ISO_code\"].strip() if \"location_ISO_code\" in row and row[\"location_ISO_code\"] else \"US\"\n",
    "\n",
    "    # Generate questions using Gemini API\n",
    "    prompt = question_prompt_template.replace(\"[Insert the claim here]\", claim)\\\n",
    "        .replace(\"[Insert the claim speaker here]\", author)\\\n",
    "        .replace(\"[Insert the claim date here]\", claim_date)\\\n",
    "        .replace(\"[Insert the location ISO code here]\", location_ISO_code)\n",
    "    \n",
    "    prompts.append(\n",
    "                    (\"chat\", \n",
    "                     prompt,\n",
    "                     claim\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "responses = batch_process(prompts, secrets_file, model_name, temperature, top_p)\n",
    "\n",
    "failed_row_ids = []\n",
    "for response in responses:\n",
    "    claim = response[1]\n",
    "    if response[0] is None:\n",
    "        failed_row_ids.append(claim)\n",
    "        continue\n",
    "\n",
    "    try:    \n",
    "        llm_decompostions = parse_gemini_response(response[0].text)\n",
    "        llm_decompostions = set([d.strip() for d in llm_decompostions])        \n",
    "        claim_queries[claim] = [(search_string, \"generated_decomposition\") \n",
    "                              for search_string in llm_decompostions]\n",
    "    except:\n",
    "        print(\"Failed to parse response for claim:\", claim)\n",
    "        print(response)\n",
    "        continue\n",
    "\n",
    "# Save claim_queries after each claim\n",
    "with open(claim_queries_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(claim_queries, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping depleted API key\n",
      "Changed Serper search SECRET,  1180  calls remaining\n"
     ]
    }
   ],
   "source": [
    "from utils.serper_customsearch import SerperCustomSearch\n",
    "serper_search = SerperCustomSearch(\"secrets/serper_secrets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2684 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 740/2684 [21:45<1:48:29,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed Serper search SECRET,  2500  calls remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1422/2684 [1:10:13<1:12:22,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed Serper search SECRET,  2500  calls remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2116/2684 [2:03:00<47:16,  4.99s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed Serper search SECRET,  2500  calls remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2684/2684 [2:50:00<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load exisitng search results\n",
    "results_filename = f\"{save_folder}/search_results_v2.json\"\n",
    "if os.path.exists(results_filename):\n",
    "    with open(results_filename, \"r\", encoding=\"utf-8\") as fp:\n",
    "        search_results = json.load(fp)\n",
    "else:\n",
    "    search_results = {}\n",
    "\n",
    "# Fetch links for each claim\n",
    "pages = 1\n",
    "for claim, queries in tqdm(claim_queries.items(), total=len(claim_queries)):\n",
    "    if claim in search_results:\n",
    "        continue\n",
    "\n",
    "    search_results[claim] = {}\n",
    "    for query in queries:\n",
    "        search_results[claim][query[0]] = serper_search.fetch_results(search_string=query[0], \n",
    "                                                                   pages_before_date=\"\", \n",
    "                                                                   location_ISO_code=\"US\", \n",
    "                                                                   n_pages=pages)\n",
    "\n",
    "    # Save search_results to file\n",
    "    with open(results_filename, \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(search_results, fp, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
