{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'claim_factcheck_url', 'claim_author', 'claim_source',\n",
      "       'claim_date', 'fact_check_date', 'justification',\n",
      "       'fact_checking_sources', 'issue', 'label'],\n",
      "      dtype='object')\n",
      "Unique claims: 2684\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv(\"data/fnd_politifact_claims.csv\")\n",
    "print(d.columns)\n",
    "\n",
    "# Delete 670\n",
    "d = d.drop(index=670).reset_index(drop=True)\n",
    "\n",
    "print(\"Unique claims:\", d.claim.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all claims with > 1 frequency\n",
    "duplicate_claims = {}\n",
    "for idx, claim in enumerate(d.claim):\n",
    "    matching_rows = d[d.claim == claim]\n",
    "    if matching_rows.shape[0] > 1:\n",
    "        row_ids = matching_rows.index.tolist()\n",
    "        parent_id = row_ids[0]\n",
    "        child_ids = row_ids[1:]\n",
    "        duplicate_claims[parent_id] = child_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_links(k, duplicate_claims):\n",
    "    return [d.iloc[k].claim_factcheck_url, [d.iloc[id].claim_factcheck_url for id in duplicate_claims[k]]]\n",
    "\n",
    "for k in duplicate_claims.keys():\n",
    "    label_start = set([d.iloc[k].fact_check_date])\n",
    "    labels = set()\n",
    "    for id in duplicate_claims[k]:\n",
    "        labels.add(d.iloc[id].fact_check_date)\n",
    "\n",
    "    if label_start != labels:\n",
    "        print(\"Discrepancy in labels for claim\", k)\n",
    "        print(label_start, labels)\n",
    "        print(fetch_links(k, duplicate_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to merge claims. \n",
    "# Combine issue into a list\n",
    "\n",
    "indices_to_drop = set()\n",
    "for k in duplicate_claims.keys():\n",
    "    issues = set([d.iloc[k].issue])\n",
    "    for duplicate_id in duplicate_claims[k]:\n",
    "        issues.add(d.iloc[duplicate_id].issue)\n",
    "        indices_to_drop.add(duplicate_id)\n",
    "    d.iloc[k].issue = str(list(issues))\n",
    "\n",
    "# Remove the duplicates\n",
    "d_dropped = d.drop(list(indices_to_drop)).reset_index(drop=True)\n",
    "len(d_dropped), d.claim.nunique()\n",
    "\n",
    "d_dropped.to_csv('data/fnd_politifact_claims_final.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2684"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/claim_queries.json\", \"r\") as f:\n",
    "    claim_queries = json.load(f)\n",
    "\n",
    "len(claim_queries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
