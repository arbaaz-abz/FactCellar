{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set home directory to parent directory    \n",
    "import json\n",
    "\n",
    "with open('data/claims_by_issue.json', 'r', encoding='utf-8') as f:\n",
    "    claims_by_issue = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trafilatura\n",
    "from trafilatura.settings import DEFAULT_CONFIG\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "def get_page(url):\n",
    "    page = None\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            page = trafilatura.fetch_url(url, config=DEFAULT_CONFIG)\n",
    "            assert page is not None\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {i+1} with trafilatura failed for {url}: {str(e)}\", file=sys.stderr)\n",
    "            sleep((i+1)*1)    \n",
    "    return page\n",
    "\n",
    "def html2lines(page):\n",
    "    if not page or len(page.strip()) == 0:\n",
    "        print(\"No page found\")\n",
    "        return {}\n",
    "\n",
    "    text = trafilatura.extract(page, \n",
    "                               favor_precision=True, \n",
    "                               with_metadata = True,    \n",
    "                               no_fallback=False,\n",
    "                               include_comments=True,\n",
    "                               output_format=\"json\"\n",
    "                            )\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing abortion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:41<00:00, 26.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing animals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:44<00:00,  7.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing border-security\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:38<00:00, 16.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing climate-change\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:47<00:00, 27.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing coronavirus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [07:52<00:00, 78.75s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing crime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:02<00:00, 40.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing corporations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:37<00:00,  6.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing children\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:30<00:00, 25.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing drugs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:21<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing economy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [02:07<02:24, 48.12s/it]Attempt 1 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n",
      "Attempt 2 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n",
      "Attempt 3 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No page found for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [05:56<00:00, 59.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing education\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:06<00:00, 31.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing energy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:00<00:00, 20.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:09<00:00, 21.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ethics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing food\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:37<00:00, 16.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing guns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:34<00:00, 25.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing health-care\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:38<00:00, 36.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing housing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:24<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing human-rights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:10<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:02<00:00, 10.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing military\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:45<00:00, 27.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing natural-disasters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:07<00:00, 11.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing welfare\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:07<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing weather\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:31<00:00, 15.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing taxes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:54<01:00, 20.06s/it]Attempt 1 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n",
      "Attempt 2 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n",
      "Attempt 3 with trafilatura failed for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No page found for https://www.politifact.com/factchecks/2022/apr/13/rebecca-kleefisch/kleefisch-didnt-have-power-cut-taxes-her-own-lieut/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [04:06<00:00, 41.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:59<00:00, 19.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sports\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:32<00:00, 25.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:49<00:00, 18.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing religion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:11<00:00, 11.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing lgbtq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:09<00:00, 21.56s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_ruling(soup):\n",
    "    ruling_text = []\n",
    "    found_ruling = False\n",
    "    article = soup.find('article', class_='m-textblock')\n",
    "    for element in article.find_all(['p', 'div']):\n",
    "        if element.name == 'div' and element.text.strip() == 'Our ruling':\n",
    "            found_ruling = True\n",
    "            continue\n",
    "        \n",
    "        # Collect all text from paragraphs after finding \"Our ruling\"\n",
    "        if found_ruling and element.name == 'p':\n",
    "            ruling_text.append(element.text.strip())\n",
    "\n",
    "    # Join all collected text with newlines\n",
    "    full_ruling = '\\n'.join(ruling_text).strip()\n",
    "    return full_ruling\n",
    "\n",
    "def get_sources(soup):\n",
    "    sources = []\n",
    "    article = soup.find('article', class_='m-superbox__content')\n",
    "    for link in article.find_all('a', href=True):\n",
    "        url = link['href']\n",
    "        # Only include external links (those starting with http)\n",
    "        if url.startswith('https'):\n",
    "            sources.append(url)\n",
    "    return sources\n",
    "\n",
    "def get_statement_source(soup):\n",
    "    desc_div = soup.find('div', class_=\"m-statement__desc\")\n",
    "    if desc_div:\n",
    "        desc_text = desc_div.text.strip()\n",
    "        # Extract text between \"in\" and \":\" using string split\n",
    "        try:\n",
    "            source = desc_text.split(\"in \")[1].split(\":\")[0].strip()\n",
    "            return source\n",
    "        except IndexError:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "# clone claims_by_issue\n",
    "claims_by_issue_clone = {}\n",
    "for idx, (issue, categories) in enumerate(claims_by_issue.items()):\n",
    "    print(f\"Processing issue: {issue}\")\n",
    "    # if issue != 'abortion':\n",
    "    #     continue\n",
    "\n",
    "    claims_by_issue_clone[issue] = {}\n",
    "    for category, claim_objects in tqdm(categories.items(), total=len(categories)):\n",
    "        claims_by_issue_clone[issue][category] = []\n",
    "        for claim_object in claim_objects:\n",
    "            statement_url = claim_object['statement_url']\n",
    "\n",
    "            # Get page\n",
    "            page = get_page(statement_url)\n",
    "            sleep(1)\n",
    "\n",
    "            if page is None:\n",
    "                print(f\"No page found for {claim_object['statement_url']}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(page, 'html.parser')\n",
    "            \n",
    "            # Get ruling/ justification\n",
    "            full_ruling = get_ruling(soup)\n",
    "\n",
    "            # Gather sources \n",
    "            sources = get_sources(soup)\n",
    "\n",
    "            # Gather claim source (Where was it spoken)\n",
    "            claim_source = get_statement_source(soup)\n",
    "\n",
    "            claim_object['justification'] = full_ruling\n",
    "            claim_object['fact_checking_sources'] = sources\n",
    "            claim_object['claim_source'] = claim_source\n",
    "\n",
    "            claims_by_issue_clone[issue][category].append(claim_object)\n",
    "\n",
    "with open('data/claims_by_issue_with_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(claims_by_issue_clone, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for issue, categories in claims_by_issue_clone.items():\n",
    "    for category, claim_objects in categories.items():\n",
    "        for claim_object in claim_objects:\n",
    "            rows.append({\n",
    "                'claim': claim_object['statement_text'],\n",
    "                'claim_factcheck_url': claim_object['statement_url'],\n",
    "                'claim_author': claim_object['author_name'],\n",
    "                'claim_source': claim_object['claim_source'],\n",
    "                'claim_date': claim_object['date_of_statement'],\n",
    "                'fact_check_date': claim_object['fact_check_date'],\n",
    "                'justification': claim_object['justification'],\n",
    "                'fact_checking_sources': claim_object['fact_checking_sources'],\n",
    "                'issue': issue,\n",
    "                'label': category,\n",
    "            })\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv('data/fnd_politifact_claims.csv', index=False, encoding='utf-8')\n",
    "print(len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
